{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2d Ising model using tensor renormalization group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# motivation\n",
    "In the course we've solved 1d Ising model using [*Transfer Matrix method*](https://en.wikipedia.org/wiki/Transfer-matrix_method):<br>\n",
    "Roughly, we treat the partition function: <br>\n",
    "<center>$Z=v_0\\cdot \\{\\prod_{k=1}^{N}W_k\\} \\cdot v_{N+1}...(1)  $</center>\n",
    "where $v_0$ and $v_{N+1}$ are first and last sides object. If we use periodic boundary condition then\n",
    "$v_{N+1}=v_0$. $W$ is the transfer matrix which connect neighbor sides object. \n",
    "As we've learned that in 1d case that there is no phase transtion since the free energy is mantained analytic. \n",
    "> *Is the phase transition happened in 2d Ising model? In the course we used mean field theory and predict there is a phase transition where $T_c=\\frac{hZJ}{K_B}$. But mean field theory may fail or get imprecise result in low dimention case. Our goal now bacome **Can we get better result than mean field prediction?** *\n",
    "\n",
    "# [*Ising model*](https://en.wikipedia.org/wiki/Ising_model)\n",
    "\n",
    "Before the project starts, we should briefly talk about the Ising model \n",
    "<a href= \"https://web.stanford.edu/~peastman/statmech/phasetransitions.html\"><img src=\"https://web.stanford.edu/~peastman/statmech/_images/ising_model.svg\" height=\"240\" width=\"240\"></a>\n",
    "\n",
    "<center> Ising model with spin  $\\pm \\frac{1}{2}$</center>\n",
    "\n",
    "Ising model is a model to describe the magnetic solid.We consider the square lattice with spin \n",
    "$\\pm \\frac{1}{2}$ and spin are only interacted by nearest neighbor. Furthermore,we can consider \n",
    "there is a external field $h$ such that enforce spin to align on specific direction(up/down).<br>\n",
    "For such model, the Hamiltonian is <br>\n",
    "\n",
    "<center> $H = -J\\sum\\limits_{<ij>}\\sigma_i\\sigma_j-h \\sum\\limits_i\\sigma_i...(2)$</center>\n",
    "where $J$ is the pair energy. $J>0$ the interaction is called ferromagnetic while $J<0$ is called antiferromagnetic. $\\sigma_i$ is the spin on $i$ site, for simplicity we just assume \n",
    "$\\sigma \\in [1,-1]$ rather than $\\in [\\frac{1}{2},-\\frac{1}{2}]$ and $h=0$ in my calculation for simplicity.\n",
    "\n",
    "# Solving 1d Ising model based on tensor network\n",
    "In [*this paper*](https://arxiv.org/abs/1201.1144), they calculated the partition function with tensor network. In my opinion, it seems a little bit like transfer matrix method with dimension more than one.Here I want to talk about transfer matrix method with tensor network briefly.<br>\n",
    "When the Hamiltonian consists only local terms with Bolzmann weight \n",
    "$W(\\{\\sigma \\}) =e^{-\\beta H\\{ \\sigma \\}}$, \n",
    "we can write the partition function as sum their sum <br>            \n",
    "<center> $Z =\\displaystyle{\\sum_{\\{\\sigma \\}}W(\\{\\sigma \\})}...(3)$ </center> <br>\n",
    "As taught in class, we know the Bolzmann weight for Ising model is\n",
    "$W(\\sigma_i,\\sigma_j)=  \n",
    "  \\begin{bmatrix}\n",
    "   W_{\\uparrow \\uparrow} & W_{\\uparrow \\downarrow}  \\\\\n",
    "   W_{\\downarrow \\uparrow} & W_{\\downarrow \\downarrow}\n",
    "  \\end{bmatrix} =\n",
    "  \\begin{bmatrix}\n",
    "   e^{\\beta} & e^{-\\beta}  \\\\\n",
    "   e^{-\\beta}& e^{\\beta}\n",
    "  \\end{bmatrix} $ (consider external field h = 0 and J = 1)<br> \n",
    "  \n",
    "The partition function then can be formulated as <br>\n",
    "\\begin{equation}\n",
    "Z = \\displaystyle\\sum_{\\{\\sigma\\}}\\otimes_{<i,j>}W_{\\sigma_i,\\sigma_j}...(4)\n",
    "\\end{equation}\n",
    "\n",
    "In the language of tensor network, matrix can be written as rank 2 tensor, so Bolzmann weight $W$ can ba drawn as \n",
    "<img src=\"http://localhost:8888/tree/notes_on_python/StatMech_project/W_tensor.png\" width=\"10%\" height=\"10%\" />\n",
    "From the Bolzmann weight, we can also draw the partition function, as the equation (1) says:<br>\n",
    "<img src= \"http://localhost:8888/tree/notes_on_python/StatMech_project/partition_function_tensor_form.png\" width=\"50%\" height=\"50%\">\n",
    "The connection between two tensors means matrix contraction. Here we assume the system has periodic boundary condition so the fist tensor contract with last one. The calculation can be down by doing similar transform, take thermodynamic limit one can see there is no phase transition happened in one dimension Ising model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# two dimensional tensor network\n",
    "Similar to 1d case,we can generate to 2d tensor network with some **unknown tensor** ( it can not apply one dimentsion transfer matrix directly, for we only have the Bolzmann weight tensor W in one dimenstion)\n",
    "<img src= \"http://localhost:8888/tree/notes_on_python/StatMech_project/partition_function\n",
    "_2d.png\" width=\"50%\" height=\"50%\">\n",
    "In [this paper](https://arxiv.org/abs/1709.07460),they have a easier way to implement partition function with tensor network in 2d/3d case.\n",
    "First they wrote Bolzmann weight W as: <br>\n",
    "<center> $ W = MM^T$ <br>\n",
    "and $M=  \n",
    "      \\begin{bmatrix}\n",
    "       \\sqrt{cosh\\beta} & \\sqrt{sinh\\beta}  \\\\\n",
    "       \\sqrt{cosh\\beta} & -\\sqrt{sinh\\beta}\n",
    "      \\end{bmatrix}...(5)$</center>\n",
    "<img src= \"http://localhost:8888/tree/notes_on_python/StatMech_project/W_tensor_sep.png\" width=\"30%\" height=\"30%\">\n",
    "And they defined a tensor\n",
    "    <center> $ A_{ijkl} = \\displaystyle\\sum_{abcd}\\delta_{abcd}M_{ai}M_{bj}M_{ck}M_{dl} ...(6)$</center>  \n",
    "<img src =  \"http://localhost:8888/tree/notes_on_python/StatMech_project/sep_2d.png\" width=\"30%\" height=\"30%\">\n",
    "Here $\\delta$ tensor(black dot in the figure above) represent high dimension Kronecker delta tenosr.By doing this way the $M$ tensor(from 1d Bolzmann wight tensor $W$) can be applied in two dimenstional cases.<br>\n",
    "So finally we have network like:\n",
    "<img src = \"http://localhost:8888/tree/notes_on_python/StatMech_project/sep_partition_function.png\" width=\"30%\" height=\"30%\">\n",
    "But in general the contraction problem is [*N-P hard*](https://en.wikipedia.org/wiki/NP-hardness)\n",
    "and in general it is impossible to solve the problem in thermodynamic lamit.Fortunately we have some ways to solve it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensor renormalization group(TRG)\n",
    "There are some ways to solve the problem, here I want to mention the technique about [**tensor renormalization group**](https://arxiv.org/abs/1201.1144).<br>\n",
    "<br>\n",
    "The spirit of TRG is to **trace out short-range effects and keep only the most important part.** Unlike the real space RG that renormalize Hamiltonian of the system, TRG refer to renormalized the whole tensor network.It is an efficient approach to approximate the tensor contraction problem we face above. By coarse-graining on each site and rescale the lattice(It is done by update the tensor we used in the algorithm), we can aprroximate the lattice size of $4^N$ in $N$ steps,which is\n",
    "converged quickly.<br><br>\n",
    "\n",
    "When doing coarse-grainging, the bond dimension would blow exponentially,so one should truncated the bond dimension at some finite number.In TRG process this is achieved by exerting some isometry matrix $U^{(n+1)}$ in it (see figure below).<br>\n",
    "<a href= \"https://arxiv.org/abs/1201.1144\"><img src=\"http://localhost:8888/notebooks/notes_on_python/StatMech_project/TRG.png\" width=\"50%\">\n",
    "</a>\n",
    "<center>steps of TRG</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyuni10'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-3bd2b5e4d847>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyuni10\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0muni10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mTRG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mchi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtimes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyuni10'"
     ]
    }
   ],
   "source": [
    "import pyuni10 as uni10 \n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "\n",
    "def TRG(beta,chi=16,times=15):\n",
    "    Delta = matD()\n",
    "    M     = matM(beta)\n",
    "    M_T   = matMT(beta)\n",
    "    T     = merge(Delta,M,M_T)\n",
    "    for iteration in range(times):\n",
    "        T2 = update(T,chi)\n",
    "        T  = update(T2,chi)\n",
    "    return tTrace(T,T,T,T)\n",
    "\n",
    "def matD():\n",
    "    \"\"\"create delta tensor\"\"\"\n",
    "    mat = np.zeros(16)\n",
    "    mat[0]=mat[15]=1\n",
    "    bdout = uni10.Bond(uni10.BD_OUT,2)\n",
    "    T   = uni10.UniTensorR([bdout,bdout,bdout,bdout])\n",
    "    T.SetRawElem(mat)\n",
    "    return T\n",
    "\n",
    "def matM(beta):\n",
    "    mat = np.array([[np.cosh(beta)**(1/2),np.sinh(beta)**(1/2)],\n",
    "                    [np.cosh(beta)**(1/2)],-1*np.sinh(beta)**(1/2)])\n",
    "    T   = uni10.UniTensorR(mat)\n",
    "    return T\n",
    "\n",
    "def matMT(beta):\n",
    "    mat = matM(beta).GetBlock()\n",
    "    T   = uni10.UniTensorR(np.transpose(mat))\n",
    "    return T\n",
    "\n",
    "def merge(delta,m,mt):\n",
    "    \"\"\" initially the tensors are different, so I decide to merge M/MT into delta tensor such that\n",
    "    the tensors are the same, therefore we only need to consider 1 tensor when doing TRG\"\"\"\n",
    "    network = uni10.Network(\"merge.net\")\n",
    "    result  = uni10.UniTensorR(delta)\n",
    "    network.PutTensor(D,delta)\n",
    "    network.PutTensor(\"M\",m)\n",
    "    network.PutTensor(\"MT\",mt)\n",
    "    network.Launch(result)\n",
    "    return result\n",
    "\n",
    "def update(T,chi):\n",
    "    T2   = CG_contraction(T,T)\n",
    "    copy = uni10.UniTensorR(T2)\n",
    "    copy.SetLabel([3,4,1,-2])\n",
    "    X    = uni10.Contract(T2,copy)\n",
    "    U,norm    =  svd(X.GetBlock(),chi)  # use SVD to truncate the tensor with bond dimension chi \n",
    "    result = iso_contract(T2,U)\n",
    "    result = uni10.Permute(result,[2,1,4,3],2)\n",
    "    return result\n",
    "\n",
    "def CG_contraction(A,B):\n",
    "    net    = uni10.Network(\"CG_contract.net\")\n",
    "    result = uni10.UniTensorR(A) \n",
    "    net.PutTensor(0,A)\n",
    "    net.PutTensor(1,B)\n",
    "    net.Launch(result)\n",
    "    result = result.CombineBond([2,-2])\n",
    "    result = result.CombineBond([4,-4])\n",
    "    return result\n",
    "\n",
    "def svd(npmat,bd,threshold=10**-8):\n",
    "    \"\"\"calling svd in numpy and truncate the matrix(npmat) with bond dimension (at most bd)\"\"\"\n",
    "    u,s,vd = np.linalg.svd(npmat)\n",
    "    bd2    = np.min([np.sum(s>=threshold),bd])\n",
    "    u      = u[:,:bd2]\n",
    "    s      = s[:bd2]\n",
    "    return u,s\n",
    "\n",
    "def iso_contract(T,isoT):\n",
    "    \"\"\" applying isometry and its transpose on tensor T, shrinkage T's dimension\"\"\"\n",
    "    iso_net = uni10.Network('isometry.net')\n",
    "    res    = uni10.UniTensorR(T)\n",
    "    isoT   = uni10.UniTensorR(isoT)\n",
    "    T_trans= uni10.UniTensorR(np.transpose(isoT.GetBlock()))\n",
    "    iso_net.PutTensor(0,T)\n",
    "    iso_net.PutTensor(1,isoT)\n",
    "    iso_net.PutTensor(2,T_trans)\n",
    "    iso_net.Launch(res)\n",
    "    return res\n",
    "\n",
    "def tTrace(Ta,Tb,Tc,Td):\n",
    "    \"\"\" return the tensor trace result\"\"\"\n",
    "    net = uni10.Network(\"trace.net\")\n",
    "    res = uni10.UniTensorR(Ta)\n",
    "    net.PutTensor(0,Ta)\n",
    "    net.PutTensor(1,Tb)\n",
    "    net.PutTensor(2,Tc)\n",
    "    net.PutTensor(3,Td)\n",
    "    net.Launch(res)\n",
    "    scalar = res.GetBlock()[0]\n",
    "    return scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
